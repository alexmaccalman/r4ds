---
title: "glm_and_rf"
author: "Alex MacCalman"
date: '`r Sys.Date()`'
output: github_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE,
                      message = FALSE, echo = TRUE, dpi = 180,
                      fig.width = 8, fig.height = 5)
library(tidyverse)
#library(silgelib)
#theme_set(theme_plex())
```


## Explore the data
```{r}
library(palmerpenguins)

penguins %>% 
        filter(!is.na(sex)) %>% 
        ggplot(aes(flipper_length_mm, bill_length_mm, color = sex, size = body_mass_g)) + 
        geom_point(alpha = 0.7) +
        facet_wrap(~species)
```

```{r}
penguins_df  <- penguins %>% 
        filter(!is.na(sex)) %>% 
        select(-year, -island)
```



## Build a model
Set up the training and testing split  
```{r}
library(tidymodels)

set.seed(123)
penguin_split <- initial_split(penguins_df, strata = sex)
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)
```
Because there is very little data, we will bootstrap resamples to get more.  
```{r}
set.seed(234)
#this is the data we wil use to make a model
penguin_boot <- bootstraps(penguin_train)
```
Now we will make some models. First we will set two model specifications.    
```{r}
#set up the engines for the two types of models
glm_spec <- logistic_reg() %>% 
        set_engine("glm")

library(ranger)
rf_spec <- rand_forest() %>% 
        set_mode("classification") %>% # here we set the model (classification or regression)
        set_engine("ranger")

```

Now we will create a pre-processor and a model. workflow allows us to put pieces of models together like lego blocks. This is a way to trian models using tidymodels.  
```{r}
#work flow sets up a preprocessor and an model
penguin_wf <- workflow() %>% 
        add_formula(sex ~ .) #this predicts sex with all the predictors. This is the preprocessor.


```
This part sets up the models.  
```{r}
#this fits a model one time  
penguin_wf %>% 
        add_model(glm_spec) %>% 
        fit(data = penguin_train) # this fits a model one time to training data

#this fits a bunch of models on the bootstrap data
glm_rs <- penguin_wf %>% 
        add_model(glm_spec) %>% 
        fit_resamples(
                resamples = penguin_boot,
        control = control_resamples(save_pred = TRUE, verbose = TRUE)
        )

rf_rs <- penguin_wf %>% 
        add_model(rf_spec) %>% 
        fit_resamples(
                resamples = penguin_boot,
        control = control_resamples(save_pred = TRUE, verbose = TRUE)
        )
```

## Evaluate modeling
Now let's evaluate the models. 
```{r}
#evaluate the glm
collect_metrics(glm_rs)
#evaluate the random forest
collect_metrics(rf_rs)
```
Both these models perform very similiarly. Beacuse teh glm is a simplier model, we choose it.  

```{r}
#make a confusion matrix to evaluate the glm
glm_rs %>% 
        conf_mat_resampled()
```
Now we will make ROC curvse ourselves.  
```{r}
glm_rs %>% 
        collect_predictions() %>% 
        group_by(id) %>%  #group by the id of each bootstrap, 25 curves
        roc_curve(sex, .pred_female) %>% 
        ggplot(aes(1 - specificity, sensitivity, color = id)) +
        geom_abline(lty = 2, color = "grey80", size = 1.5) +
        geom_path(show.legend = FALSE, alpha = 0.6, size = 1.2) +
        coord_equal()
```

Now we are going to use our test data. The last_fit funtion emulates the process where, after determining the best model, the final fit on the entire training set is needed and is then evaluated on the test set. 
```{r}
penguin_final <- penguin_wf %>% 
        add_model(glm_spec) %>% 
        last_fit(penguin_split) #use the original split data

penguin_final
```
Here we look at some metrics on the final model.  
```{r}
collect_metrics(penguin_final) #these collect metrics on the testing data
collect_predictions(penguin_final) #this looks at the predictions
collect_predictions(penguin_final) %>% conf_mat(sex, .pred_class) #creates a confusion matrix of true data with predicted data
```

Now we will extract a fitted model workflow.  
```{r}
penguin_final$.workflow[[1]] %>% 
        tidy() #this tidy's the workflow so we can see the coefficient estimates

#here we tidy the coefficient estimates with exponential in order to interpret the estimates as odds ratios.
penguin_final$.workflow[[1]] %>% 
        tidy(exponentiate = TRUE) %>% #this tidy's the workflow so we can see the coefficient estimates
        arrange(estimate) # order the coeeficient estimates to see the highest impact
        
```
bill depth has highes impact. 1 mm increase in bill depth equates to ~4 times higher odds of being male.  
For every one gram increase (body_mass_g) in body mass there is a 1% increase in odds of a penguin being male (estiamtes is 1.01). 0.01 is hwere we get 1%.  

Now lets plot with the bill depth to see its impact on distinguishing between male and female penguins.  

```{r}
penguins %>% 
        filter(!is.na(sex)) %>% 
        ggplot(aes(bill_depth_mm, bill_length_mm, color = sex, size = body_mass_g)) + 
        geom_point(alpha = 0.7) +
        facet_wrap(~species)
```
We can see the bill depth does make the males stand out from the females.  
