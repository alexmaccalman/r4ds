---
title: "xgboost"
author: "Alex MacCalman"
date: "8/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## what makes a team more likely to win
Our modeling goal is to predict whether a beach volleyball team of two won their match based on game play stats like errors, blocks, attacks, etc.  
```{r}
vb_matches <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-19/vb_matches.csv', guess_max = 76000)
library(tidyverse)
#reshape the data, combine both players stats
vb_parsed <- vb_matches %>%
  transmute(
    circuit,
    gender,
    year,
    w_attacks = w_p1_tot_attacks + w_p2_tot_attacks,
    w_kills = w_p1_tot_kills + w_p2_tot_kills,
    w_errors = w_p1_tot_errors + w_p2_tot_errors,
    w_aces = w_p1_tot_aces + w_p2_tot_aces,
    w_serve_errors = w_p1_tot_serve_errors + w_p2_tot_serve_errors,
    w_blocks = w_p1_tot_blocks + w_p2_tot_blocks,
    w_digs = w_p1_tot_digs + w_p2_tot_digs,
    l_attacks = l_p1_tot_attacks + l_p2_tot_attacks,
    l_kills = l_p1_tot_kills + l_p2_tot_kills,
    l_errors = l_p1_tot_errors + l_p2_tot_errors,
    l_aces = l_p1_tot_aces + l_p2_tot_aces,
    l_serve_errors = l_p1_tot_serve_errors + l_p2_tot_serve_errors,
    l_blocks = l_p1_tot_blocks + l_p2_tot_blocks,
    l_digs = l_p1_tot_digs + l_p2_tot_digs
  ) %>%
  na.omit()
#make one row per outcome. Make two data frames and bin them together
winners <- vb_parsed %>% 
        select(circuit, gender, year,
               w_attacks:w_digs) %>% 
        rename_with(~ str_remove_all(., "w_"), w_attacks:w_digs) %>% 
        mutate(win = "win")
losers <- vb_parsed %>% 
        select(circuit, gender, year,
               l_attacks:l_digs) %>% 
        rename_with(~ str_remove_all(., "l_"), l_attacks:l_digs) %>% 
        mutate(win = "lose")

vb_df <- bind_rows(winners, losers) %>% 
        mutate_if(is.character, factor)
```
## Explore the data  
```{r}
vb_df %>%
  pivot_longer(attacks:digs, names_to = "stat", values_to = "value") %>%
  ggplot(aes(gender, value, fill = win, color = win)) +
  geom_boxplot(alpha = 0.4) +
  facet_wrap(~stat, scales = "free_y", nrow = 2) +
  labs(y = NULL, color = NULL, fill = NULL)
```
## Build a Model
```{r}
library(tidymodels)

set.seed(123)
vb_split <- initial_split(vb_df, strata = win)
vb_train <- training(vb_split)
vb_test <- testing(vb_split)

#make model specification
xgb_spec <- boost_tree(
        trees = 1000,
        tree_depth = tune(), min_n = tune(), loss_reduction = tune(),
        sample_size = tune(), mtry = tune(),
        learn_rate = tune()
) %>% 
        set_engine("xgboost") %>% 
        set_mode("classification")

#set up what values we will try
xgb_grid <- grid_latin_hypercube(
        tree_depth(),
        min_n(),
        loss_reduction(),
        sample_size = sample_prop(), 
        finalize(mtry(), vb_train),
        learn_rate(),
        size = 20
)
#set up a workflow
xgb_wf <- workflow() %>% 
        add_formula(win ~ .) %>% 
        add_model(xgb_spec)

#need data to tune on
set.seed(123)
vb_folds <- vfold_cv(vb_train, strata = win)

#set up parallel processing
doParallel::registerDoParallel()

set.seed(234)
starttime <- Sys.time()
xbg_res<- tune_grid(
        xgb_wf,
        resamples = vb_folds,
        grid = xgb_grid,
        control = control_grid(save_pred = TRUE)
)
endtime <- Sys.time()
tottime <- endtime - starttime
```
## Explore the results.  
```{r}
# make a visualization
xbg_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")
    
```
# find the best
```{r}
show_best(xbg_res, "roc_auc")

best_auc <- select_best(xbg_res, "roc_auc")

# finalize the workflow
final_xgb <- finalize_workflow(xgb_wf, best_auc)
#fit the final model ad do variable importance
library(vip)
final_xgb %>% 
        fit(data = vb_train) %>% 
        pull_workflow_fit() %>% 
        vip(geom = "point")
```
pull out the final model
```{r}
# Fit the final best model to the training set and evaluate the test set
final_res <- last_fit(final_xgb, vb_split)
final_res %>% 
        collect_metrics()

final_res %>% 
        collect_predictions() %>% 
        conf_mat(win, .pred_class)

final_res %>% 
        collect_predictions() %>% 
        roc_curve(win, .pred_lose) %>% 
        autoplot()


```


